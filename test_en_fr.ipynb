{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch.utils.data import TensorDataset\n",
    "from dataset import Dataset\n",
    "import torch\n",
    "import io\n",
    "import unicodedata\n",
    "import re\n",
    "from train import Trainer\n",
    "from functools import partial\n",
    "from models import *\n",
    "import torch\n",
    "import utils\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exit.\n",
      "finish loading dataset\n"
     ]
    }
   ],
   "source": [
    "# load train dataset to get vocabulary\n",
    "ds = Dataset(corp_paths=('./data/train.lc.norm.tok.en', './data/train.lc.norm.tok.fr'),\n",
    "             emb_paths=('./data/wiki.multi.en.vec', './data/wiki.multi.fr.vec'),\n",
    "             pairs_paths=('./data/full_en_fr.npy', './data/full_fr_en.npy'),\n",
    "             max_length=20, test_size=0.1)\n",
    "print('finish loading dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set and preprosess\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "src_test_path = './data/val.lc.norm.tok.en'\n",
    "tgt_test_path = './data/val.lc.norm.tok.fr'\n",
    "\n",
    "with io.open(src_test_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "    src_sents = list(map(normalize_string, f.readlines()))\n",
    "\n",
    "with io.open(tgt_test_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "    tgt_sents = list(map(normalize_string, f.readlines()))\n",
    "\n",
    "assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "src_sentences = [ds.vocabs['src'].get_indices(sentence, language='src', pad=0) for sentence in src_sents]\n",
    "tgt_sentences = [ds.vocabs['tgt'].get_indices(sentence, language='tgt', pad=0) for sentence in tgt_sents]\n",
    "\n",
    "raw_src_sents = []\n",
    "raw_tgt_sents = []\n",
    "test_sentences1 = []\n",
    "test_sentences2 = []\n",
    "\n",
    "for i in range(len(src_sentences)):\n",
    "    if len(src_sentences[i]) <= 20 and len(tgt_sentences[i]) <= 20:\n",
    "        test_sentences1.append(src_sentences[i])\n",
    "        test_sentences2.append(tgt_sentences[i])\n",
    "        raw_src_sents.append(src_sents[i])\n",
    "        raw_tgt_sents.append(tgt_sents[i])\n",
    "        \n",
    "src_sentences = test_sentences1\n",
    "tgt_sentences = test_sentences2\n",
    "src_sents = raw_src_sents\n",
    "tgt_sents = raw_tgt_sents\n",
    "\n",
    "# print(src_sents[10])\n",
    "# print(tgt_sents[10])\n",
    "# max_lenth = max([len(s) for s in tgt_sentences])\n",
    "# print(max_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(\n",
      "      (embedding): Embedding(11244, 300)\n",
      "    )\n",
      "    (rnn): LSTM(300, 150, num_layers=3, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embedding): Embedding(\n",
      "      (embedding): Embedding(11581, 300)\n",
      "    )\n",
      "    (attn): Linear(in_features=600, out_features=20, bias=False)\n",
      "    (attn_sm): Softmax(dim=1)\n",
      "    (attn_out): Linear(in_features=600, out_features=300, bias=False)\n",
      "    (attn_out_relu): ReLU()\n",
      "    (rnn): LSTM(300, 300, num_layers=3, dropout=0.3)\n",
      "    (generator): Generator(\n",
      "      (out): Linear(in_features=300, out_features=11581, bias=True)\n",
      "      (sm): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Seq2Seq(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(\n",
      "      (embedding): Embedding(11581, 300)\n",
      "    )\n",
      "    (rnn): LSTM(300, 150, num_layers=3, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embedding): Embedding(\n",
      "      (embedding): Embedding(11244, 300)\n",
      "    )\n",
      "    (attn): Linear(in_features=600, out_features=20, bias=False)\n",
      "    (attn_sm): Softmax(dim=1)\n",
      "    (attn_out): Linear(in_features=600, out_features=300, bias=False)\n",
      "    (attn_out_relu): ReLU()\n",
      "    (rnn): LSTM(300, 300, num_layers=3, dropout=0.3)\n",
      "    (generator): Generator(\n",
      "      (out): Linear(in_features=300, out_features=11244, bias=True)\n",
      "      (sm): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "src2tgt = torch.load('./saved_models/en-fr/' + str(num_epochs) + '/' + str(num_epochs) + '_word_to_word_1_' + str(num_epochs - 1) + '.src2tgt.pt').to(device)\n",
    "tgt2src = torch.load('./saved_models/en-fr/' + str(num_epochs) + '/' + str(num_epochs) + '_word_to_word_1_' + str(num_epochs - 1) + '.tgt2src.pt').to(device)\n",
    "print(src2tgt)\n",
    "print(tgt2src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, batch, length, l1='src', l2='tgt', n_iters=None):\n",
    "    sos_index = ds.get_sos_index(l1)\n",
    "    eos_index = ds.get_eos_index(l2)\n",
    "    results = model.evaluate(batch.to(device),length.to(device), sos_index, eos_index, n_iters=n_iters)\n",
    "    results = utils.log_probs2indices(results)\n",
    "    results = ds.visualize_batch(results, l2)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the translations of the test set\n",
    "pad_index = {}\n",
    "pad_index['src'] = ds.get_pad_index('src')\n",
    "pad_index['tgt'] = ds.get_pad_index('tgt')\n",
    "# src_sentences\n",
    "\n",
    "s_length, s_padded_batches = utils.pad_monolingual_batch(src_sentences, pad_index['src'])\n",
    "s_data = TensorDataset(torch.tensor(s_length, dtype=torch.long),torch.tensor(s_padded_batches, dtype=torch.long))\n",
    "s_data_iter = torch.utils.data.DataLoader(s_data, batch_size=32, shuffle=False, pin_memory=True)\n",
    "src_predictions = []\n",
    "for i, batch in enumerate(s_data_iter):\n",
    "    length = batch[0]\n",
    "    b = batch[1].transpose(0, 1)\n",
    "    src_predictions += predict(src2tgt, b, length, n_iters=20)\n",
    "    \n",
    "t_length, t_padded_batches = utils.pad_monolingual_batch(tgt_sentences, pad_index['tgt'])\n",
    "t_data = TensorDataset(torch.tensor(t_length, dtype=torch.long),torch.tensor(t_padded_batches, dtype=torch.long))\n",
    "t_data_iter = torch.utils.data.DataLoader(t_data, batch_size=32, shuffle=False, pin_memory=True)\n",
    "tgt_predictions = []\n",
    "for i, batch in enumerate(t_data_iter):\n",
    "    length = batch[0]\n",
    "    b = batch[1].transpose(0, 1)\n",
    "    tgt_predictions += predict(tgt2src, b, length, l1='tgt', l2='src', n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2759952747081595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# compare src_to_tgt\n",
    "predictions = [[w for w in sentence.strip().split()] for sentence in src_predictions]\n",
    "references = [[[w for w in sentence.strip().split()]] for sentence in tgt_sents]\n",
    "# reference = references[0]\n",
    "# candidate = predictions[0]\n",
    "bleu_scores = [sentence_bleu(references[i], predictions[i], weights=(0.5, 0.5)) for i in range(len(predictions))]\n",
    "print(sum(bleu_scores) / len(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3126098050650645\n"
     ]
    }
   ],
   "source": [
    "# compare tgt_to_src\n",
    "predictions = [[w for w in sentence.strip().split()] for sentence in tgt_predictions]\n",
    "references = [[[w for w in sentence.strip().split()]] for sentence in src_sents]\n",
    "# reference = references[0]s\n",
    "# candidate = predictions[0]\n",
    "bleu_scores = [sentence_bleu(references[i], predictions[i], weights=(0.5, 0.5)) for i in range(len(predictions))]\n",
    "print(sum(bleu_scores) / len(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'man', 'in', 'a', 'black', 'wetsuit', 'is', 'surfing', 'on', 'a', 'wave', '.']]\n",
      "['a', 'man', 'in', 'a', 'wetsuit', 'surfing', 'a', 'wave', '.']\n"
     ]
    }
   ],
   "source": [
    "reference = references[50]\n",
    "candidate = predictions[50]\n",
    "print(reference)\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
